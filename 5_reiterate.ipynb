{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:20px'><b> Dataset re-evaluation </b></p>\n",
    "<p>\n",
    "\n",
    "Based on the modelling implemented, following are the best result obtained on test data so far: \n",
    "\n",
    "Model A) Log-reg, l1 penalty, alpha = 3.25e-6 | Micro-avg F1-score = 56.0%, Macro-avg F1-score = 43.2%\n",
    "\n",
    "Model B) Lin-SVM, l1 penalty, alpha = 3.25e-6 | Micro-avg F1-score = 57.1%, Macro-avg F1-score = 42.1%\n",
    "\n",
    "- In both cases, the Macro-avg score is quite low compared to the Micro-avg F1 score.\n",
    "- This implies that the F1-score for the less frequent tags is quite lower that the top tags which may be due low datapoints having these tags since the points were sampled randomply.\n",
    "- To rectify this, we will now sample the data to capture atleast 1000 datapoints for each of the 500 tags to be predicted.\n",
    "    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block execution time:  0:06:39.320949\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Load the Train.csv dataset for Stack over flow tagging problem \n",
    "raw_data = pd.read_csv('dataset/Train.csv')\n",
    "\n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px'><b> Old dataset comparison (1.5L points)  </b></p>\n",
    "\n",
    "<p>\n",
    "\n",
    "Plotting the number of data points associated to each tag in the old dataset havng 1.5L datapoints. \n",
    "    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block execution time:  0:00:05.098217\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Load the trimmed data set of 1.25L datapoints for preprocessing\n",
    "raw_data_old = pd.read_csv('dataset/trimmed.csv')\n",
    "\n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of unique tags:  500\n",
      "\n",
      "2. Number of questions associated to a tag: \n",
      " [  116   121   123   124   124   124   126   128   129   129   129   130\n",
      "   130   131   134   134   135   136   136   136   137   137   137   138\n",
      "   138   138   139   139   139   139   140   140   140   141   141   142\n",
      "   142   142   142   143   144   144   145   145   145   145   146   146\n",
      "   146   147   147   147   147   147   147   147   148   148   149   149\n",
      "   149   150   150   150   151   152   154   154   154   155   156   156\n",
      "   156   157   157   157   157   158   158   158   158   158   158   158\n",
      "   159   160   160   160   160   160   160   160   161   161   161   161\n",
      "   162   162   162   163   163   164   165   165   165   166   166   166\n",
      "   167   167   167   168   168   168   168   168   168   169   169   171\n",
      "   171   172   172   172   172   172   173   174   175   175   176   176\n",
      "   176   177   177   178   178   178   178   178   178   178   179   179\n",
      "   180   180   180   180   181   181   181   182   183   183   183   183\n",
      "   184   185   185   185   185   186   187   187   188   188   189   189\n",
      "   189   191   191   191   192   193   194   195   196   196   197   198\n",
      "   199   199   200   200   201   202   202   202   203   203   203   205\n",
      "   207   212   213   215   215   215   215   216   216   217   217   218\n",
      "   218   218   219   219   219   220   220   221   221   221   222   223\n",
      "   224   224   227   227   228   230   231   232   233   235   236   237\n",
      "   238   239   240   241   241   241   241   241   242   244   244   244\n",
      "   244   245   246   246   247   249   251   252   258   261   261   262\n",
      "   263   265   265   266   266   267   271   272   273   273   274   274\n",
      "   275   275   277   277   278   279   280   281   282   286   288   289\n",
      "   289   291   291   293   296   298   299   301   301   301   303   305\n",
      "   306   307   310   310   311   311   311   313   319   322   322   322\n",
      "   324   326   327   328   330   335   335   336   338   340   342   343\n",
      "   343   345   345   346   347   349   350   353   356   356   356   357\n",
      "   359   359   360   363   364   366   367   369   370   374   374   374\n",
      "   377   377   377   378   379   379   382   382   385   386   387   388\n",
      "   388   393   395   396   401   402   403   405   410   415   418   419\n",
      "   424   426   427   428   431   431   435   435   435   436   437   438\n",
      "   438   443   450   451   454   454   458   458   463   465   475   478\n",
      "   479   480   483   484   487   494   507   510   517   517   518   522\n",
      "   523   527   528   529   537   541   544   546   556   557   563   563\n",
      "   571   575   584   588   598   608   608   613   619   624   627   630\n",
      "   640   641   650   687   699   703   705   734   736   737   755   755\n",
      "   757   776   790   793   793   802   804   810   826   837   842   852\n",
      "   863   864   865   878   889   890   902   925   936   937   978   995\n",
      "  1028  1032  1039  1120  1132  1166  1269  1273  1302  1318  1370  1376\n",
      "  1431  1433  1534  1640  1681  1689  1709  1810  1816  2128  2140  2438\n",
      "  2708  3056  3230  3723  3753  3760  4090  4594  4692  4941  4967  5001\n",
      "  5306  5630  8794  9337 10445 11158 11763 13231]\n",
      "\n",
      "3. Lowest number of questions associated to a tag:  116\n",
      "\n",
      "Block execution time:  0:00:02.281066\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# List to store each tagging instance accross all datapoints\n",
    "tag_list = []\n",
    "\n",
    "tags = [str(x).split(\" \") for x in raw_data_old.Tags_shortlist]\n",
    "\n",
    "for datapoint in tags:\n",
    "    for tag in datapoint:\n",
    "        tag_list.append(tag)\n",
    "\n",
    "# Calculating the unique tags & their frequency in a descending order from the above tag_list        \n",
    "unique_tags, tag_count = np.unique(tag_list, return_counts=True)\n",
    "sort_indices = np.argsort(tag_count)[::-1]\n",
    "unique_tags = unique_tags[sort_indices]\n",
    "\n",
    "print(\"1. Number of unique tags: \", len(unique_tags))\n",
    "\n",
    "cvt = CountVectorizer(analyzer = lambda x:x)\n",
    "ques_tag_map = cvt.fit_transform(tags)\n",
    "tag_vocab = cvt.vocabulary_\n",
    "\n",
    "_, tag_count = np.unique(ques_tag_map.nonzero()[1], return_counts=True)\n",
    "tag_count.sort()\n",
    "\n",
    "print(\"\\n2. Number of questions associated to a tag: \\n\", tag_count)\n",
    "\n",
    "print(\"\\n3. Lowest number of questions associated to a tag: \", np.min(tag_count))\n",
    "\n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(raw_data_old.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px'><b> Inferences (Old data)  </b></p>\n",
    "\n",
    "<p>\n",
    "\n",
    "- Total number of points in the dataset = 150000\n",
    "- Lowest number & % of questions associated to a tag = 116, 0.0073%\n",
    "    \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px'><b> New data (Tag based Even sampling)  </b></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block execution time:  0:17:16.971711\n"
     ]
    }
   ],
   "source": [
    "# drop the duplicate rows and create a new dataset\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "raw_data = raw_data.drop_duplicates(subset = ['Title', 'Body', 'Tags'])\n",
    "\n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block execution time:  0:00:12.082068\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "tags = [str(x).split(\" \") for x in raw_data.Tags]\n",
    "\n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block execution time:  0:00:17.675524\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# List to store each tagging instance accross all datapoints\n",
    "tag_list = []\n",
    "\n",
    "for datapoint in tags:\n",
    "    for tag in datapoint:\n",
    "        tag_list.append(tag)\n",
    "\n",
    "# Calculating the unique tags & their frequency in a descending order from the above tag_list        \n",
    "unique_tags, tag_count = np.unique(tag_list, return_counts=True)\n",
    "sort_indices = np.argsort(tag_count)[::-1]\n",
    "unique_tags = unique_tags[sort_indices]\n",
    "\n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block execution time:  0:00:13.428589\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Creating a sparse representation of the datapoint/question number and associated tags\n",
    "cvt = CountVectorizer(analyzer = lambda x:x)\n",
    "ques_tag_map = cvt.fit_transform(tags)\n",
    "tag_vocab = cvt.vocabulary_\n",
    "    \n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(475777, 4)\n",
      "\n",
      "Block execution time:  0:01:37.715584\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Reducing data size for adjusting to PC computational limitations\n",
    "\n",
    "# Filtering out the datapoints that don't have any of the top 500 tags as the associated tags\n",
    "num_top_tags = 500\n",
    "top_tags = unique_tags[:num_top_tags]\n",
    "tag_inds = [tag_vocab[tag] for tag in top_tags]\n",
    "relev_ques = ques_tag_map[:, tag_inds]\n",
    "\n",
    "minq_per_tag = 1000\n",
    "minq_ind = set()\n",
    "\n",
    "for i in range(num_top_tags):\n",
    "    \n",
    "    ques = relev_ques[:, i].nonzero()[0]\n",
    "    temp = min(minq_per_tag, len(ques))\n",
    "    rand_inds = random.sample(range(0, len(ques)), temp)\n",
    "    minq_ind.update(ques[rand_inds])\n",
    "    \n",
    "trimmed_data = raw_data.iloc[list(minq_ind)]\n",
    "print(trimmed_data.shape)\n",
    "    \n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block execution time:  0:00:15.116112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tejas Chavan\\AppData\\Local\\Temp\\ipykernel_5688\\2259576672.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trimmed_data['Tags_shortlist'] = shortlist_tags\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Creating a new column name Tags_shortlist that contains only the top 500 tags from the Tags column\n",
    "tags_temp = [str(x).split(\" \") for x in trimmed_data.Tags]\n",
    "shortlist_tags = [[item for item in datapoint if item in top_tags] for datapoint in tags_temp]\n",
    "shortlist_tags = [\" \".join(item) for item in shortlist_tags]\n",
    "trimmed_data['Tags_shortlist'] = shortlist_tags\n",
    "\n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of unique tags:  500\n",
      "\n",
      "2. Number of questions associated to a tag: \n",
      " [ 1105  1132  1135  1140  1148  1149  1150  1152  1154  1157  1160  1161\n",
      "  1161  1162  1163  1163  1163  1165  1167  1173  1179  1182  1183  1191\n",
      "  1191  1196  1196  1198  1198  1203  1204  1206  1206  1209  1209  1210\n",
      "  1215  1216  1219  1220  1221  1225  1228  1229  1230  1232  1236  1239\n",
      "  1243  1245  1246  1249  1249  1251  1251  1251  1252  1252  1254  1254\n",
      "  1254  1255  1258  1259  1260  1260  1260  1261  1261  1262  1262  1263\n",
      "  1263  1264  1264  1266  1266  1269  1270  1272  1273  1273  1274  1275\n",
      "  1275  1276  1277  1277  1277  1278  1280  1280  1283  1283  1285  1287\n",
      "  1288  1288  1289  1291  1294  1295  1296  1298  1298  1299  1300  1300\n",
      "  1301  1301  1302  1302  1303  1304  1305  1307  1309  1310  1311  1311\n",
      "  1313  1315  1316  1318  1321  1321  1322  1322  1323  1324  1327  1327\n",
      "  1327  1328  1333  1333  1334  1337  1339  1340  1342  1343  1345  1346\n",
      "  1347  1349  1350  1351  1352  1353  1355  1356  1356  1359  1364  1364\n",
      "  1366  1368  1371  1372  1372  1372  1373  1373  1374  1377  1378  1378\n",
      "  1378  1380  1384  1385  1388  1389  1392  1393  1395  1395  1395  1397\n",
      "  1397  1400  1400  1401  1402  1402  1403  1404  1406  1407  1408  1412\n",
      "  1413  1415  1415  1419  1420  1423  1424  1426  1430  1431  1433  1434\n",
      "  1437  1438  1439  1441  1444  1445  1445  1447  1448  1448  1449  1449\n",
      "  1450  1451  1452  1455  1459  1459  1460  1461  1469  1469  1472  1474\n",
      "  1476  1480  1480  1482  1483  1484  1487  1489  1490  1490  1491  1492\n",
      "  1492  1494  1495  1497  1498  1498  1499  1503  1508  1508  1514  1518\n",
      "  1518  1519  1520  1522  1523  1523  1525  1528  1528  1530  1531  1535\n",
      "  1536  1538  1538  1540  1542  1544  1546  1549  1560  1560  1561  1574\n",
      "  1575  1575  1576  1576  1578  1582  1583  1584  1585  1597  1599  1600\n",
      "  1602  1603  1605  1607  1608  1618  1619  1625  1626  1630  1634  1641\n",
      "  1644  1650  1650  1651  1655  1662  1662  1662  1665  1666  1669  1690\n",
      "  1691  1701  1702  1704  1714  1716  1720  1726  1732  1733  1737  1741\n",
      "  1743  1747  1760  1762  1779  1786  1787  1787  1787  1792  1792  1796\n",
      "  1807  1819  1836  1836  1837  1838  1840  1841  1844  1845  1846  1846\n",
      "  1846  1866  1889  1897  1903  1909  1912  1913  1922  1922  1924  1929\n",
      "  1937  1943  1946  1947  1955  1955  1962  1964  1965  1969  1972  1978\n",
      "  1978  1982  1984  1995  1996  2004  2009  2011  2018  2020  2020  2021\n",
      "  2023  2025  2035  2038  2056  2058  2065  2069  2109  2112  2119  2126\n",
      "  2135  2168  2174  2183  2189  2195  2209  2228  2278  2286  2290  2307\n",
      "  2313  2329  2331  2339  2344  2390  2434  2436  2470  2487  2525  2533\n",
      "  2569  2585  2588  2593  2596  2601  2626  2652  2656  2670  2727  2754\n",
      "  2783  2804  2838  2905  3012  3014  3050  3052  3083  3126  3136  3140\n",
      "  3141  3147  3164  3182  3206  3226  3259  3284  3338  3349  3365  3366\n",
      "  3373  3425  3448  3515  3576  3595  3608  3776  3833  3974  4029  4087\n",
      "  4408  4410  4500  4632  4839  5007  5366  5380  5530  5628  6771  7185\n",
      "  8085  8296  8627  9161  9697  9866 10346 11165 11210 11928 12926 13658\n",
      " 14067 18293 18390 19762 24071 28707 34847 35181]\n",
      "\n",
      "3. Lowest number of questions associated to a tag:  1105\n",
      "\n",
      "Block execution time:  0:00:03.187918\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# List to store each tagging instance accross all datapoints\n",
    "tag_list = []\n",
    "\n",
    "tags = [str(x).split(\" \") for x in trimmed_data.Tags_shortlist]\n",
    "\n",
    "for datapoint in tags:\n",
    "    for tag in datapoint:\n",
    "        tag_list.append(tag)\n",
    "\n",
    "# Calculating the unique tags & their frequency in a descending order from the above tag_list        \n",
    "unique_tags, tag_count = np.unique(tag_list, return_counts=True)\n",
    "sort_indices = np.argsort(tag_count)[::-1]\n",
    "unique_tags = unique_tags[sort_indices]\n",
    "\n",
    "print(\"1. Number of unique tags: \", len(unique_tags))\n",
    "\n",
    "cvt = CountVectorizer(analyzer = lambda x:x)\n",
    "ques_tag_map = cvt.fit_transform(tags)\n",
    "tag_vocab = cvt.vocabulary_\n",
    "\n",
    "_, tag_count = np.unique(ques_tag_map.nonzero()[1], return_counts=True)\n",
    "tag_count.sort()\n",
    "\n",
    "print(\"\\n2. Number of questions associated to a tag: \\n\", tag_count)\n",
    "\n",
    "print(\"\\n3. Lowest number of questions associated to a tag: \", np.min(tag_count))\n",
    "\n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='font-size:16px'><b> Inferences (New data)  </b></p>\n",
    "\n",
    "<p>\n",
    "\n",
    "- Total number of points in the dataset = 475777\n",
    "- Lowest number & % of questions associated to a tag = 1105, 0.232%\n",
    "    \n",
    "Compared to the old data, the tag with lowest frequency still accounts for 0.232 datapoints compared to 0.0773% in the old dataset. \n",
    "\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Block execution time:  0:01:09.076848\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Save data to csv file\n",
    "#### Imp: Careful not to overwrite data ###\n",
    "\n",
    "try:\n",
    "    trimmed_data.to_csv('dataset 5L/trimmed.csv', mode = 'w', index = False)\n",
    "except:\n",
    "    print(\"File already saved?\")\n",
    "\n",
    "print(\"\\nBlock execution time: \", datetime.datetime.now() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
